{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 单层前向神经网络例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28805666.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 12367.09375\n100 446.92889404296875\n150 29.964210510253906\n200 2.4932303428649902\n250"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.22652092576026917\n300 0.02163032628595829\n350 0.002363927662372589\n400 0.00042014956125058234\n450"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.00013166134885977954\n500 5.862200850970112e-05\n"
     ]
    }
   ],
   "source": [
    "# 样本数，样本维度，隐藏层维度，输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10 \n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, requires_grad = True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for i in range(501):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if i % 50 == 0:\n",
    "        print(i, loss.item())\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= w1.grad*learning_rate\n",
    "        w2 -= w2.grad*learning_rate\n",
    "        # grad清零  因为已经为W权重重新赋值了。\n",
    "        # 否则下次反向传播时又会带上上一次的grad结果\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用nn包构建前向网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 720.2109985351562\n50 41.37998580932617\n100 3.235888957977295\n150 0.3981344997882843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.06371669471263885\n250 0.011966506950557232\n300 0.00247399415820837\n350 0.0005454039201140404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.00012585466902237386\n450 3.0133418476907536e-05\n500 7.42580914447899e-06\n"
     ]
    }
   ],
   "source": [
    "# 样本数，样本维度，隐藏层维度，输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10 \n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out)\n",
    ")\n",
    "loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for i in range(501):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(i, loss.item())\n",
    "        \n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for para in model.parameters():\n",
    "            para -= para.grad * learning_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用optim去更新权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 714.3883666992188\n50 227.57684326171875\n100 61.03059005737305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 10.047301292419434\n200 0.9512631893157959\n250 0.059015870094299316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.0026670400984585285\n350 9.352606866741553e-05\n400 2.5314604954473907e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 4.5081094413035316e-08\n500 7.876985175236939e-10\n"
     ]
    }
   ],
   "source": [
    "# 样本数，样本维度，隐藏层维度，输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10 \n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out)\n",
    ")\n",
    "loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "for i in range(501):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(i, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 690.1822509765625\n50 34.1402702331543\n100 2.139177083969116\n150 0.23636604845523834\n200 0.037243906408548355\n250"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.007162435445934534\n300 0.0015055050607770681\n350 0.00033183288178406656\n400 7.519089558627456e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 1.7330039554508403e-05\n500 4.043296485178871e-06\n"
     ]
    }
   ],
   "source": [
    "# 自定义双层模型\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H)\n",
    "        self.linear2 = nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "        \n",
    "        \n",
    "# 样本数，样本维度，隐藏层维度，输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10 \n",
    "# 随机生成样本 标签\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "learning_rate = 1e-4\n",
    "# 实例化模型\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
    "\n",
    "for i in range(501):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(i, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 607.8623657226562\n50 162.10675048828125\n100 66.3708267211914\n150 2.579711437225342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 4.121944427490234\n250 4.304188251495361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 16.872217178344727\n350 5.7509942054748535\n400 1.5938948392868042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 0.6730867028236389\n500 0.4437660276889801\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        在构造函数中，我们构造了三个nn.Linear实例，它们将在前向传播时被使用。\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        对于模型的前向传播，我们随机选择0、1、2、3，\n",
    "        并重用了多次计算隐藏层的middle_linear模块。\n",
    "        由于每个前向传播构建一个动态计算图，\n",
    "        我们可以在定义模型的前向传播时使用常规Python控制流运算符，如循环或条件语句。\n",
    "        在这里，我们还看到，在定义计算图形时多次重用同一个模块是完全安全的。\n",
    "        这是Lua Torch的一大改进，因为Lua Torch中每个模块只能使用一次。\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "# 样本数，样本维度，隐藏层维度，输出层维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10 \n",
    "# 随机生成样本 标签\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "learning_rate = 1e-4\n",
    "# 实例化模型\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum=0.8)\n",
    "\n",
    "for i in range(501):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(i, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
